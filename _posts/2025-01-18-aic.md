---
layout: post
title: Entropy measure I - information storage
date: 2025-01-18 21:40:00-0800
description: understanding information storage
tags: entropy
categories: research
giscus_comments: false
related_posts: false
---

To better understand _entropy_, a measure of time-series complexity, I decided to read more articles. Here I summarize what I understand from [Xiong et al. (2017)](https://doi.org/10.1103/PhysRevE.95.062114).

According to the article (p.4), **information storage** is defined as below:

> "Another relevant entropy measure is the so-called information storage, which quantifies the amount of information _shared between the present and the past observations of the considered stochastic process_."

The formula is described as below:

$S(X) = I(X_{n};X_{n}^-)\n = \mathop{\mathbb{E}}[log\frac{p(x_{1},...,x_{n}}{p(x_{1},...,x_{n-1})p(x_{n})}]$

$I(X_{n};X_{n}^-) denotes the mutual information between $X_{n}$ and $X_{n}^-$. Also, $X_{n}^- = [X_{1},...,X_{n-2},X_{n-1}]$.

