---
layout: post
title: Entropy measure I - information storage
date: 2025-01-17 19:40:00-0800
description: understanding information storage
tags: entropy
categories: research
giscus_comments: false
related_posts: false
---

To better understand _entropy_, a measure of time-series complexity, I decided to read more articles. Here I summarize what I understand from [Xiong et al. (2017)](https://doi.org/10.1103/PhysRevE.95.062114).

According to the article (p.4), **information storage** is defined as below:

> "Another relevant entropy measure is the so-called information storage, which quantifies the amount of information _shared between the present and the past observations of the considered stochastic process_."

The formula is described as below:

$S(X) = I(X_{n};X_{n}^-) = \mathop{\mathbb{E}}[log\frac{p(x_{1},...,x_{n}}{p(x_{1},...,x_{n-1})p(x_{n})}]$

$I(X_{n};X_{n}^-)$ denotes the mutual information between $X_{n}$ and $X_{n}^-$. Also, $X_{n}^- = [X_{1},...,X_{n-2},X_{n-1}]$.

Here I try to implement this in Python. There is already a Python package: [`PyInform`](https://elife-asu.github.io/PyInform/timeseries.html). I have to study this one later.

To begin, eq.(14) of the article: $p(x_{n}) = \frac{1}{N}\sum_{i=1}^NK(\lVert x_{n}-x_{i} \rVert)$ is prepared. $\lVert . \rVert$ will be `Chebyshev distance` and the kernel $K$ will be a `Heaviside` kernel with a threshold $r$, as introduced in the article.

```
def heaviside(x, r):
  return 1 if x<r else 0

def prob(series, x_n, r):
  """ series: a time series
      x_n: current state
  """
  Nseries = len(series)  # this is equal to N in eq.(14)
  out = sum([heaviside(abs(x_n-x_i), r) for x_i in series])/Nseries
  return out
```

To implement eq.(17), we need to expand the `prob` function to handle a vector input, $x_{n}^m$.

```
def prob(series, x_n, r):
  """ series: a time series
      x_n: a vector
  """
  Nseries = len(series)  # this is NOT equal to N in eq.(14)
  m = len(x_n)  # this is equal to the superscript m in x_{n}^m
  N = Nseries - m + 1   # this is equal to N in eq.(14)
  sumcount = 0
  for k in range(N):
    x_i = series[k:k+m]
    sumcount += heaviside(max(abs(x_n - x_i)), r)
  return sumcount/N_i
```

There's one more layer though. Ultimately, each $p(x_{n}^m)$, $p(x_{n})$, and $p(x_{n}, x_{n}^m)$ needs to undergo $<.>$,
