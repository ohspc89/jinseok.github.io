---
layout: post
title: Entropy measure I - information storage
date: 2025-01-17 19:40:00-0800
description: understanding information storage
tags: entropy
categories: research
giscus_comments: false
related_posts: false
---

To better understand _entropy_, a measure of time-series complexity, I decided to read more articles. Here I summarize what I understand from [Xiong et al. (2017)](https://doi.org/10.1103/PhysRevE.95.062114).

According to the article (p.4), **information storage** is defined as below:

> "Another relevant entropy measure is the so-called information storage, which quantifies the amount of information _shared between the present and the past observations of the considered stochastic process_."

The formula is described as below:

$$
(X) = I(X_{n};X_{n}^-) = \mathop{\mathbb{E}}[log\frac{p(x_{1},...,x_{n})}{p(x_{1},...,x_{n-1})p(x_{n})}]
$$

$$I(X_{n};X_{n}^-)$$ denotes the mutual information between $$X_n$$ and $$X_{n}^-$$. Also, $$X_{n}^- = [X_{1},...,X_{n-2},X_{n-1}]$$.

Here I try to implement this in Python. There is already a Python package: [`PyInform`](https://elife-asu.github.io/PyInform/timeseries.html). I have to study this one later.

To begin, eq.(14) of the article: $$p(x_{n}) = \frac{1}{N}\sum_{i=1}^NK(\lVert x_{n}-x_{i} \rVert)$$ is prepared. $$\lVert . \rVert$$ will be `Chebyshev distance` and the kernel $$K$$ will be a `Heaviside` kernel with a threshold $$r$$, as introduced in the article.

```python
def heaviside(x, r):
  return 1 if x<r else 0

def prob(series, x_n, r):
  """ series: a time series
      x_n: current state
      r: a threshold of the kernel
  """
  N = len(series)  # this is equal to N in eq.(14)
  out = sum([heaviside(abs(x_n-x_i), r) for x_i in series])/N
  return out
```

To implement eq.(17), we need to expand the `prob` function to handle a vector input, $$x_n^m$$.

$$
S(X) = I(X_n; X_n^m) = \ln\frac{\langle p(x_n, x_n^m) \rangle}{\langle p(x_n) p(x_n^m) \rangle}
$$

```python
def prob(series, x_n, r):
  """ series: a time series
      x_n: a vector or a float
      r: threshold
  """
  Nseries = len(series)  # this is NOT equal to N in eq.(14)
  m = len(x_n)  # this is equal to the superscript m in x_{n}^m
  if m > 1:
    N = Nseries - m + 1   # this is equal to N in eq.(14)
    sumcount = 0
    for k in range(N):
      x_i = series[k:k+m]
      sumcount += heaviside(max([abs(xn-xi) for xn, xi in zip(x_n, x_i)]), r)
    return sumcount/N_i
  else:
    sumcount = sum([heaviside(abs(x_n-x_i), r) for x_i in series])/Nseries
    return sumcount/N_i
```

$$\langle .\rangle$$ denotes the average taken over all possible $$x_{n}$$. How would I implement that? I need to know the range of $$n$$. What are the possible values of $$n$$? If the length of a time series is $$N$$ then $$n$$ should vary from $$m+1$$ to $$N$$.

Another question of mine: is $$p(x_n, x_n^m)$$ equal to $$p(x_n)*p(x_n^m)$$?

```python
import numpy as np

def info_storage(series, m, r):
  """ series: a time series
      m: dimension
      r: threshold
  """
  Nseries = len(series)
  numer_joints = []
  denom_joints = []
  for n in range(m, Nseries):
    x_n = series[m]
    x_nm = series[1+n-m:m]
    numer_joint = prob(x_n)*prob(x_nm)
    numer_joints.append(numer_joint)
    denom_joints.append(prob(x_nm)
  numer = sum(numer_joints)/len(numer_joints)  # <.>
  denom = sum(denom_joints)/len(denom_joints)
  return -np.log(numer/denom)
```

To check if my implementation is correct, I have asked mighty ChatGPT. Here I provide a jupyer notebook showing the results.
