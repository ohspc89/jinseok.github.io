{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Steps I follow:\n",
        "\n",
        "1. Create an AR(2) time series.\n",
        "2. Calculate information storage based on my method\n",
        "3. Calculate information storage based on ChatGPT's method\n",
        "4. Compare the results!\n",
        "\n",
        "Full disclosure - I wrote the code first by myself and asked ChatGPT to refine it."
      ],
      "metadata": {
        "id": "IgX_1SG_9yF6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yWjNQ9YQ9j3O"
      },
      "outputs": [],
      "source": [
        "# Load packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1. Create a time series: AR(2)\n",
        "def generate_time_series(phi1: float,\n",
        "                         phi2: float,\n",
        "                         sigma: float,\n",
        "                         n: int,\n",
        "                         seed: int=42) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generates a synthetic AR(2) time series.\n",
        "\n",
        "    Parameters:\n",
        "    phi1 (float): AR coefficient for lag 1.\n",
        "    phi2 (float): AR coefficient for lag 2.\n",
        "    sigma (float): Standard deviation of noise.\n",
        "    n (int): Length of the time series.\n",
        "    seed (int): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "    np.ndarray: The generated time series.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    epsilon = np.random.normal(0, sigma, n) # noise\n",
        "    ts = np.zeros(n)  # Preallocate the time series array\n",
        "    for t in range(2, n):\n",
        "      ts[t] = phi1 * ts[t-1] + phi2 * ts[t-2] + epsilon[t]\n",
        "    return ts\n",
        "\n",
        "# AR(2) parameters\n",
        "PHI1 = 0.4\n",
        "PHI2 = -0.3\n",
        "SIGMA = 1   # Standard deviation of the white noise\n",
        "N_TS = 100  # Length of the time series\n",
        "\n",
        "my_tseries = generate_time_series(PHI1, PHI2, SIGMA, N_TS)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2-1. Heaviside kernel with radius 'r'\n",
        "def my_heaviside(x: float, r: float) -> int:\n",
        "  \"\"\"\n",
        "  Heaviside step function for threshold comparison.\n",
        "\n",
        "  Parameters:\n",
        "  x (float): Input value.\n",
        "  r (float): Threshold value.\n",
        "\n",
        "  Returns:\n",
        "  int: 1 if x <= r, else 0.\n",
        "  \"\"\"\n",
        "  return 1 if x <= r else 0"
      ],
      "metadata": {
        "id": "jEktOxumdnIH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step was to write a function that calculates the kernel density estimate. I had to modify it multiple times working with ChatGPT. That's a typical learning process... but it took me a LOT of time.\n",
        "\n",
        "`prob` can calculate $p(x_n)$, the probability of a single value $x_n$ or $p(x_n^m)$, that of a vector. Furthermore, it can calculate $p(x_n, x_n^m)$, the joint probability."
      ],
      "metadata": {
        "id": "Sb1J4DaDduZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2-2. Kernel density estimate\n",
        "def prob(series: list | np.ndarray,\n",
        "         x_n: int | float | np.ndarray | list,\n",
        "         r: float,\n",
        "         joint: bool = False) -> float:\n",
        "  \"\"\"\n",
        "  computes the probability for a given value or vector in the series.\n",
        "\n",
        "  Parameters:\n",
        "  series (list or np.ndarray): The input time series.\n",
        "  x_n (int, float, np.ndarray or list): The value or vector for probability calculation.\n",
        "  r (float): Radius for the Heaviside kernel.\n",
        "  joint (bool): Whether to compute joint probability.\n",
        "\n",
        "  Returns:\n",
        "  float: The calculated probability.\n",
        "  \"\"\"\n",
        "  if isinstance(x_n, (int, float)) and joint:\n",
        "    raise ValueError(\"Set 'joint=False' for a scalar x_n\")\n",
        "\n",
        "  series_len = len(series)\n",
        "  p = 1 if isinstance(x_n, (int, float)) else len(x_n)\n",
        "\n",
        "  if p > 1:\n",
        "    iterend = series_len-p+1 if joint else series_len-p\n",
        "    sumcount = sum(\n",
        "        my_heaviside(\n",
        "            max(abs(xn-xi) for xn, xi in zip(x_n, series[k:k+p])),\n",
        "            r\n",
        "        )\n",
        "        for k in range(iterend)\n",
        "    )\n",
        "    return sumcount/iterend\n",
        "  sumcount = sum(my_heaviside(abs(x_n-x_i), r) for x_i in series) / series_len\n",
        "  return sumcount"
      ],
      "metadata": {
        "id": "heBDs37dnCjm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So `prob` starts with checking if `x_n` is a scalar (`int` or `float`) and `joint=True`. For a scalar you CANNOT calculate joint probaility.\n",
        "\n",
        "Depending on the value of `p`, the length of `x_n`, the return of the function is different.\n",
        "\n",
        "**_p_ = 1**\n",
        "\n",
        "The last two lines correspond to `p = 1`,\n",
        "$p(x_n) = \\frac{1}{N}\\sum_{i=1}^N K(||x_n - x_i||)$. Here, $N$ is the length of a time series.\n",
        "\n",
        "**_p_ > 1**\n",
        "\n",
        "If `p > 1`, the subscript $i$ has different ranges depending on which probability is calculated. If $p(x_n^m)$ is calculated, $x_n^m$ cannot include the very last element of `series`. Remember, $x_n^m = (x_{n-1}, ..., x_{n-m})$.\n",
        "\n",
        "Let's use a simple example to understand. Suppose $x = (0,1,2,3,4,5,6)$ and $m = 2$. How do we calculate $p(x_n^m)$ for $n=3$ for example?\n",
        "\n",
        "First, $x_3^2 = (x_{3-1}, x_{3-2}) = (1,0)$.\n",
        "\n",
        "Then we need to find $x_i^m$ to calculate $p(x_n^m)$. Xiong et al. (2017) does not have an equation for it, but we can tweak eq.(14). Realizations of $x_i^m$ will be $(1, 0), (2,1), (3,2), (4,3), (5,4)$ for $i = 3, 4, 5, 6, 7$. Note that $(6,5)$ is not a possible value because this is $x_8^2$. However, there is no $x_8$, so $x_8^2$ is impossible.\n",
        "\n",
        "Each of these differences will be the input to the kernel function:\n",
        "$max(|(1,0) - (1,0)|); max(|(1,0) - (2,1)|); max(|(1,0) - (3,2)|); max(|(1,0) - (4,3)|); max(|(1,0) - (5,4)|)$.\n",
        "\n",
        "Let's set $r$, the Heaviside function's threshold, as 1.5. Then $K(.)$ will be $(1, 1, 0, 0, 0)$. Check eq.(18) of the article to understand why this is. Then $p(x_3^2) =\\frac{1}{5}\\sum_{i=3}^7 K(||x_n^m - x_i^m||) = 2/5$. The value of the denomenator is 5. This value is coming from \"7 - 2\" which is `series_len - p`.\n",
        "\n",
        "If $p(x_n, x_n^m)$ is calculated (`joint=True`), $(x_n, x_n^m) = (x_n, x_{n-1},...,x_{n-m})$ is a m+1 vector. In our example, $(x_3, x_3^2) = (2,1,0)$. Five corresponding $(x_i, x_i^m)$ values will be $(2,1,0), (3,2,1), (4,3,2), (5,4,3), (6,5,4)$. Finally, $p(x_3, x_3^2) = \\frac{1}{5}\\sum_{i=3}^7 K(||(x_n, x_n^m) - (x_i, x_i^m)||) = (1+1+0+0+0)/5.$ The denominator is again 5. This value is equal to 7 - 2, but this is NOT `series_len - p` which is 7 - 3 = 4. In fact, it is `series_len - p + 1`.\n",
        "\n",
        "Consequently, to handle both cases in one function, the denominator (`iterend`) is set conditionally. It is `series_len - p + 1` for `joint=True` and `series_len - p` for `joint=False`.\n",
        "\n",
        "I admit that this is somewhat erroneous, because $p(x_n^m)$ is also a **joint** probability distribution: $p(x_{n-1},...,x_{n-m})$. It works though."
      ],
      "metadata": {
        "id": "2WLcEjvOuPJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2-3. Integrate everything to calculate the kernel density estimate\n",
        "def my_information_storage(series: list | np.ndarray,\n",
        "                           m: int,\n",
        "                           r: float) -> float:\n",
        "  \"\"\"\n",
        "  Computes the information storage of a time series.\n",
        "\n",
        "  Parameters:\n",
        "  series (list or np.ndarray): The input time series.\n",
        "  m (int): Embedding dimension.\n",
        "  r (float): Radius for Heaviside kernel.\n",
        "\n",
        "  Returns:\n",
        "  float: The computed information storage value.\n",
        "  \"\"\"\n",
        "  series_length = len(series)\n",
        "  numerator = sum(\n",
        "      prob(series, series[n-m:n+1], r, joint=True)\n",
        "      for n in range(m, series_length)\n",
        "      ) / (series_length - m)\n",
        "  denominator = sum(\n",
        "      prob(series, series[n-m:n], r) * prob(series, series[n], r)\n",
        "      for n in range(m, series_length)\n",
        "      ) / (series_length-m)\n",
        "  return np.log(numerator/denominator)"
      ],
      "metadata": {
        "id": "Yc_zTC3-u0V6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`numerator` is $\\langle p(x_n, x_n^m) \\rangle$ and `denominator` is $\\langle p(x_n)*p(x_n^m) \\rangle$."
      ],
      "metadata": {
        "id": "sAmHt4jyirkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2-4. Run my implementation\n",
        "# Let's try for m = 2, r = 0.2 * std(series)\n",
        "embedding_dim = 2\n",
        "radius = 0.2*np.std(my_tseries)\n",
        "S_X = my_information_storage(my_tseries, embedding_dim, radius)\n",
        "print(f\"My Information Storage: {S_X:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcjzb8wmo2AG",
        "outputId": "c8085441-2afe-450b-b7ae-b956cfd3d042"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My Information Storage: 1.5866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I provide ChatGPT's version. This also went though many edits. My lesson was that answers of ChatGPT should be validated, ALL THE TIME."
      ],
      "metadata": {
        "id": "ZT5wwp6G38jN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3-1. ChatGPT's prep functions\n",
        "\n",
        "def heaviside_kernel(distance, r):\n",
        "  \"\"\"Heaviside kernel function.\"\"\"\n",
        "  return 1 if distance <= r else 0\n",
        "\n",
        "def chebyshev_distance(x, y):\n",
        "  \"\"\"Compute Chebyshev distance.\"\"\"\n",
        "  return np.max(np.abs(x-y))\n",
        "\n",
        "\n",
        "def information_storage_gpt(x_series, m, r):\n",
        "  N = len(x_series)\n",
        "  if N <= m:\n",
        "    raise ValueError(\"The length of the time series must be greater than the embedding dimension m.\")\n",
        "\n",
        "  # Calculate marginal densities\n",
        "  p_x = calculate_kernel_density(x_series, r)\n",
        "\n",
        "  # Construct embedding vectors for p(x_n^m)\n",
        "  x_embedded = np.array([x_series[i-m:i+1][::-1] for i in range(m, N)])\n",
        "\n",
        "  # Calculate joint densities\n",
        "  p_joint = calculate_joint_density(x_series, m, r)\n",
        "\n",
        "  # Calculate p(x_n^m)\n",
        "  p_x_m = calculate_kernel_density(x_embedded, r)\n",
        "\n",
        "  # Compute Information Storage\n",
        "  S_X = 0\n",
        "  for n in range(m, N):\n",
        "    # previously it was `p_joint[n-m] > 0`\n",
        "    if p_joint > 0 and p_x[n] > 0 and p_x_m[n-m] > 0:\n",
        "      S_X += np.log(p_joint / (p_x[n]*p_x_m[n-m]))\n",
        "  S_X /= (N-m)\n",
        "\n",
        "  return S_X"
      ],
      "metadata": {
        "id": "2c6al5EupCG4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGPT prepares two separate functions: `calculate_kernel_density` and `calculate_joint_density`. The former is equal to my `prob` with a scalar `x_n`. The latter is the same as my `prob` with a vector `x_n` & `joint=True`."
      ],
      "metadata": {
        "id": "62dhpjnclfgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3-2. ChatGPT's density estimation, part I\n",
        "def calculate_kernel_density(x_series, r, m=None):\n",
        "  \"\"\"\n",
        "  Estimate the probability density p(x_n) or p(x_n^m)\n",
        "  using kernel density estimation.\n",
        "\n",
        "  Parameters:\n",
        "  x_series (ndarray): Time series data (length N).\n",
        "  r (float): Radius for the Heaviside kernel.\n",
        "  m (int, optional): Embedding dimension (number of lags).\n",
        "                     If None, calculates for p(x_n).\n",
        "\n",
        "  Returns:\n",
        "  ndarray: Probability densities for each x_n or x_n^m.\n",
        "  \"\"\"\n",
        "  N = len(x_series)\n",
        "  start_index = m if m else 0\n",
        "  p = np.zeros(N - start_index)\n",
        "  for n in range(start_index, N):\n",
        "    for i in range(N):\n",
        "      distance = chebyshev_distance(x_series[n], x_series[i])\n",
        "      p[n - start_index] += heaviside_kernel(distance, r)\n",
        "    p[n - start_index] /= N\n",
        "  return p\n",
        "\n",
        "def calculate_joint_density(x_series, m, r):\n",
        "  \"\"\"\n",
        "  Estimate the joint probability density <p(x_n, x_n^m)> using kernel density estimation.\n",
        "\n",
        "  Parameters:\n",
        "    x_series (ndarray): Time series data (length N)\n",
        "    m (int): Embedding dimension (number of lags)\n",
        "    r (float): Radius for the Heaviside kernel\n",
        "\n",
        "  Returns:\n",
        "    float: The expected joint density <p(x_n, x_n^m)>\n",
        "  \"\"\"\n",
        "  N = len(x_series)\n",
        "  if N <= m:\n",
        "    raise ValueError(\"Time series length must be greater than the embedding dimension.\")\n",
        "\n",
        "  # Create all lagged vectors [x_i, x_i^m]\n",
        "  lagged_vectors = np.array([x_series[i-m:i+1][::-1] for i in range(m, N)]) # Shape: (N-m, m+1)\n",
        "\n",
        "  # Joint density estimation\n",
        "  joint_density_sum = 0\n",
        "  for x_target in lagged_vectors:\n",
        "    density = 0\n",
        "    for x_candidate in lagged_vectors:\n",
        "      distance = chebyshev_distance(x_target, x_candidate)\n",
        "      density += heaviside_kernel(distance, r)\n",
        "    joint_density_sum += density / len(lagged_vectors) # Normalize inner loop\n",
        "\n",
        "  # Normalize outer loop\n",
        "  return joint_density_sum / len(lagged_vectors)"
      ],
      "metadata": {
        "id": "w4FbKwgvlel1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGPT also prepares another function, `calculate_expected_product`. This was done after so many frustrating attempts of mine..."
      ],
      "metadata": {
        "id": "NiLZdnH0mJBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3-3. ChatGPT's density estimation, part II\n",
        "def calculate_expected_product(x_series, m, r):\n",
        "  \"\"\"\n",
        "  Calculate <p(x_n) * p(x_n^m)>.\n",
        "\n",
        "  Parameters:\n",
        "  x_series (ndarray): Time series data (length N).\n",
        "  m (int): Embedding dimension (number of lags).\n",
        "  r (float): Radius for the Heaviside kernel.\n",
        "\n",
        "  Returns:\n",
        "  float: The expected value <p(x_n)*p(x_n^m)>,\n",
        "         calculated over all valid n in the time series\n",
        "  \"\"\"\n",
        "  N = len(x_series)\n",
        "  if N <= m:\n",
        "    raise ValueError(\"Time series length must be greater than the embedding dimension.\")\n",
        "\n",
        "  # Calculate p(x_n)\n",
        "  p_x = calculate_kernel_density(x_series, r, m=0)\n",
        "\n",
        "  # Create lagged vectors for p(x_n^m)\n",
        "  lagged_vectors = np.array([x_series[i-m:i][::-1] for i in range(m, N)]) # Shape: (N-m, m)\n",
        "  p_x_m = np.zeros(N-m) # Preallocate memory\n",
        "  for n, x_target in enumerate(lagged_vectors):\n",
        "    for x_candidate in lagged_vectors:\n",
        "      distance = chebyshev_distance(x_target, x_candidate)\n",
        "      p_x_m[n] += heaviside_kernel(distance, r)\n",
        "    p_x_m[n] /= len(lagged_vectors)\n",
        "\n",
        "  # Calculate the expected product\n",
        "  expected_product = 0\n",
        "  for n in range(N-m):\n",
        "    expected_product += p_x[n+m] * p_x_m[n]\n",
        "\n",
        "  return expected_product / (N-m)"
      ],
      "metadata": {
        "id": "IWB-EJYfmn-F"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3-4. Everything integrated\n",
        "def gpt_information_storage(x_series, m, r):\n",
        "  N = len(x_series)\n",
        "  if N <= m:\n",
        "    raise ValueError(\"The length of the time series must be greater than the embedding dimension m.\")\n",
        "\n",
        "  # Calculate joint densities\n",
        "  p_joint = calculate_joint_density(x_series, m, r)\n",
        "\n",
        "  # Calculate <p(x_n) * p(x_n^m)>\n",
        "  expected_product = calculate_expected_product(x_series, m, r)\n",
        "\n",
        "  # Compute Information Storage\n",
        "  S_X = np.log(p_joint / expected_product)\n",
        "\n",
        "  return S_X"
      ],
      "metadata": {
        "id": "-_kDlJUbn_uZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3-5. Run ChatGPT implementation\n",
        "\n",
        "# Same configuration\n",
        "gpt_S_X = gpt_information_storage(my_tseries, embedding_dim, radius)\n",
        "print(f\"GPT's Information Storage: {gpt_S_X:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOnSpRL71Q4X",
        "outputId": "119bd9df-6e1b-4e00-c211-a1c495101bf6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT's Information Storage: 1.5866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both return $S(X)$ = 1.5866.\n",
        "\n",
        "I initially started with Python's basic features. ChatGPT mostly handles data using NumPy's ndarray.\n",
        "\n",
        "In terms of structure, I handled everything in one function, `prob`. ChatGPT provides a modular fashion. `calculate_kernel_density` is used in `calculate_expected_product`. There is a separate `calculate_joint_density`. Next time I can mimic this behavior for better management.\n",
        "\n",
        "Finally, ChatGPT's method is more _correct_ in how it prepares $x_n^m = (x_{n-1},...,x_{n-m})$. In different functions, `lagged_vectors` is prepared, and the segments of the original time series are 'flipped' by the command: `[::-1]`. I have not done this, and in terms of calculation it does not matter."
      ],
      "metadata": {
        "id": "AhPdSwMwpi6L"
      }
    }
  ]
}